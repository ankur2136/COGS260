Answer2 

a)

iteration 0 :: loss 2.5940543069617954
iteration 10 :: loss 1.66027152005014
iteration 20 :: loss 1.3408146796519456
iteration 30 :: loss 1.0293583112820996
iteration 40 :: loss 0.8434751018711674
iteration 50 :: loss 0.747715206574926
iteration 60 :: loss 0.6987483957552709
iteration 70 :: loss 0.6579854632260809
iteration 80 :: loss 0.6185117688765488
iteration 90 :: loss 0.5897201472236744
iteration 100 :: loss 0.5687253541322465
iteration 110 :: loss 0.5524859496572386
iteration 120 :: loss 0.5392808783351006
iteration 130 :: loss 0.5281741406866634
iteration 140 :: loss 0.5186001356868121
iteration 150 :: loss 0.5102307359885822
iteration 160 :: loss 0.5028184896310421
iteration 170 :: loss 0.4961893892974345
iteration 180 :: loss 0.49022575256837625
iteration 190 :: loss 0.4848201614362613
iteration 200 :: loss 0.4798862678931431
iteration 210 :: loss 0.475354803739916
iteration 220 :: loss 0.471175296456505
iteration 230 :: loss 0.4673045674986106
iteration 240 :: loss 0.46370211714815845
iteration 250 :: loss 0.4603233746841243
iteration 260 :: loss 0.4571476715072709
iteration 270 :: loss 0.4541567913321532
iteration 280 :: loss 0.45133241433388627
iteration 290 :: loss 0.4486535172108886
iteration 300 :: loss 0.44609411247272157
iteration 310 :: loss 0.44365353662347806
iteration 320 :: loss 0.4413200319134444
iteration 330 :: loss 0.43909048458474337
iteration 340 :: loss 0.43695330974705554
iteration 350 :: loss 0.4348931330999727
iteration 360 :: loss 0.4329021847800325
iteration 370 :: loss 0.43098028498585794
iteration 380 :: loss 0.4291233287697773
iteration 390 :: loss 0.4273325229489547
iteration 400 :: loss 0.4255995357224782
iteration 410 :: loss 0.423918910324068
iteration 420 :: loss 0.42229181494793266
iteration 430 :: loss 0.42070874225000643
iteration 440 :: loss 0.41916367266852356
iteration 450 :: loss 0.41765853956633214
iteration 460 :: loss 0.41618350311877217
iteration 470 :: loss 0.41473708432110135
iteration 480 :: loss 0.41332068746520206
iteration 490 :: loss 0.4119344726963093
iteration 500 :: loss 0.4105747171711387
iteration 510 :: loss 0.40924496801571775
iteration 520 :: loss 0.4079404701803976
iteration 530 :: loss 0.4066606365237418
iteration 540 :: loss 0.4054052208658429
iteration 550 :: loss 0.4041691236355091
iteration 560 :: loss 0.4029549880371662
iteration 570 :: loss 0.4017637597845452
iteration 580 :: loss 0.40059312335624137
iteration 590 :: loss 0.39944038114555047
iteration 600 :: loss 0.39830549449054325
iteration 610 :: loss 0.3971889954871366
iteration 620 :: loss 0.3960876644488256
iteration 630 :: loss 0.39499899247465053
iteration 640 :: loss 0.3939236972321455
iteration 650 :: loss 0.3928600317589662
iteration 660 :: loss 0.39181076772707624
iteration 670 :: loss 0.39077627796294134
iteration 680 :: loss 0.38975656235719164
iteration 690 :: loss 0.38874756455256765
iteration 700 :: loss 0.3877499429289797
iteration 710 :: loss 0.38676181998001913
iteration 720 :: loss 0.3857827478753326
iteration 730 :: loss 0.3848144245621961
iteration 740 :: loss 0.3838563424816015
iteration 750 :: loss 0.38291032761117877
iteration 760 :: loss 0.38197362304006177
iteration 770 :: loss 0.3810447974880842
iteration 780 :: loss 0.3801279410454709
iteration 790 :: loss 0.3792199839536094

test accuracy: 0.9289
train accuracy: 0.9275166666666667




ans2 
c
Enable Regularisation
teration 0 :: loss 2.5539534461679545
iteration 10 :: loss 1.6313889902747785
iteration 20 :: loss 1.2019307095735217
iteration 30 :: loss 1.0968373184364104
iteration 40 :: loss 0.8601724983039786
iteration 50 :: loss 0.7758204139959332
iteration 60 :: loss 0.7128831396369336
iteration 70 :: loss 0.6640028986859877
iteration 80 :: loss 0.6289109610553592
iteration 90 :: loss 0.6017164314354142
iteration 100 :: loss 0.5797504883988531
iteration 110 :: loss 0.5617962099903131
iteration 120 :: loss 0.546983635685346
iteration 130 :: loss 0.5345898903189659
iteration 140 :: loss 0.5240416779440714
iteration 150 :: loss 0.5148818906142197
iteration 160 :: loss 0.5068108025921895
iteration 170 :: loss 0.49962506380535654
iteration 180 :: loss 0.49317793414227185
iteration 190 :: loss 0.48734633169204444
iteration 200 :: loss 0.4820328584328741
iteration 210 :: loss 0.4771649426651492
iteration 220 :: loss 0.47268617212471076
iteration 230 :: loss 0.4685397254841197
iteration 240 :: loss 0.4646805363670775
iteration 250 :: loss 0.46107184503718246
iteration 260 :: loss 0.4576895829047506
iteration 270 :: loss 0.4545132028452016
iteration 280 :: loss 0.4515140240393578
iteration 290 :: loss 0.4486734256743632
iteration 300 :: loss 0.44597569426608163
iteration 310 :: loss 0.44340889474581463
iteration 320 :: loss 0.44095797378247326
iteration 330 :: loss 0.43861600163334724
iteration 340 :: loss 0.43637333717899346
iteration 350 :: loss 0.43421886967071793
iteration 360 :: loss 0.4321469820263454
iteration 370 :: loss 0.4301483134720614
iteration 380 :: loss 0.4282191349155418
iteration 390 :: loss 0.4263578411541684
iteration 400 :: loss 0.42455481942640616
iteration 410 :: loss 0.4228078679023216
iteration 420 :: loss 0.4211115372498858
iteration 430 :: loss 0.4194606397130204
iteration 440 :: loss 0.4178503494719301
iteration 450 :: loss 0.4162810952169328
iteration 460 :: loss 0.4147505770002262
iteration 470 :: loss 0.41325250716633066
iteration 480 :: loss 0.41178385886495483
iteration 490 :: loss 0.410349220446125
iteration 500 :: loss 0.4089467696505919
iteration 510 :: loss 0.40757157975116093
iteration 520 :: loss 0.4062248077796355
iteration 530 :: loss 0.40490372338954833
iteration 540 :: loss 0.4036087860456844
iteration 550 :: loss 0.4023337561119883
iteration 560 :: loss 0.40108233583437036
iteration 570 :: loss 0.39985031981821334
iteration 580 :: loss 0.3986392852042768
iteration 590 :: loss 0.39745025918744953
iteration 600 :: loss 0.3962779917723118
iteration 610 :: loss 0.39511796501097246
iteration 620 :: loss 0.3939734192776393
iteration 630 :: loss 0.3928443255314937
iteration 640 :: loss 0.3917286043973377
iteration 650 :: loss 0.3906283328743386
iteration 660 :: loss 0.3895393003237656
iteration 670 :: loss 0.3884636255237982
iteration 680 :: loss 0.3874015714373552
iteration 690 :: loss 0.38635015090117175
iteration 700 :: loss 0.3853089122855
iteration 710 :: loss 0.3842789036481645
iteration 720 :: loss 0.3832584770651674
iteration 730 :: loss 0.38224668105630377
iteration 740 :: loss 0.3812426090890759
iteration 750 :: loss 0.3802483114163307
iteration 760 :: loss 0.37926327164357426
iteration 770 :: loss 0.3782831523242724
iteration 780 :: loss 0.3773103932277719
iteration 790 :: loss 0.37634352782217007
iteration 800 :: loss 0.37538926899040487
iteration 810 :: loss 0.3744446043827684
iteration 820 :: loss 0.3735055404262717
iteration 830 :: loss 0.37257381118089106
iteration 840 :: loss 0.3716501585502838
iteration 850 :: loss 0.37073262543658025
iteration 860 :: loss 0.3698231876238034
iteration 870 :: loss 0.3689219083954388
iteration 880 :: loss 0.36802662721227686
iteration 890 :: loss 0.36713667382815196
iteration 900 :: loss 0.366254140154223
iteration 910 :: loss 0.3653775848866629

test accuracy: 0.9291
train accuracy: 0.9275


Enable both regularisation and momentum

iteration 0 :: loss 2.5617856828048344
iteration 10 :: loss 0.8260308025030689
iteration 20 :: loss 0.5699418559155486
iteration 30 :: loss 0.5069271381037032
iteration 40 :: loss 0.4664178145926738
iteration 50 :: loss 0.44050862192365337
iteration 60 :: loss 0.42325927092178206
iteration 70 :: loss 0.4109677528359804
iteration 80 :: loss 0.40076065820463447
iteration 90 :: loss 0.3916850082414576
iteration 100 :: loss 0.38326191771064244
iteration 110 :: loss 0.3752715185813964
iteration 120 :: loss 0.36760581935444947
iteration 130 :: loss 0.36022365924770094
iteration 140 :: loss 0.35308793355572077
iteration 150 :: loss 0.3462090066700228
iteration 160 :: loss 0.3396011643188224
iteration 170 :: loss 0.3332456783249395
iteration 180 :: loss 0.32714432594901105
iteration 190 :: loss 0.3212799579318193
iteration 200 :: loss 0.3156635527124032
iteration 210 :: loss 0.31028521273220977
iteration 220 :: loss 0.3051471876291743
iteration 230 :: loss 0.30025261279512994
iteration 240 :: loss 0.29557983157260803
iteration 250 :: loss 0.29112958484072776
iteration 260 :: loss 0.28687891797368215
iteration 270 :: loss 0.28281533082474386
iteration 280 :: loss 0.2789268865399298
iteration 290 :: loss 0.2752069388712537
iteration 300 :: loss 0.27164157543487233
iteration 310 :: loss 0.26822510604535804
iteration 320 :: loss 0.2649454502084874
iteration 330 :: loss 0.2617982136344377
iteration 340 :: loss 0.25877246595974773
iteration 350 :: loss 0.2558534246926922
iteration 360 :: loss 0.2530383346256793
iteration 370 :: loss 0.2503234178217866
iteration 380 :: loss 0.24770871013803183
iteration 390 :: loss 0.24519634158700299
iteration 400 :: loss 0.24278012560295464
iteration 410 :: loss 0.24045559914938783
iteration 420 :: loss 0.23821436268312282
iteration 430 :: loss 0.23605355169999329
iteration 440 :: loss 0.2339592939810218
iteration 450 :: loss 0.2319293009798825
iteration 460 :: loss 0.2299678883917235
iteration 470 :: loss 0.2280719314036368
iteration 480 :: loss 0.2262417193739471
iteration 490 :: loss 0.22446768452242583
iteration 500 :: loss 0.22274727625029428
iteration 510 :: loss 0.22108067862256076
iteration 520 :: loss 0.2194652315896563
iteration 530 :: loss 0.2179007587834758
iteration 540 :: loss 0.21638419537084314
iteration 550 :: loss 0.21491594644338755
iteration 560 :: loss 0.21349284553455836
iteration 570 :: loss 0.21211086749885494
iteration 580 :: loss 0.210770828759722
iteration 590 :: loss 0.20946939016922017
iteration 600 :: loss 0.20820578323365566
iteration 610 :: loss 0.2069778013105359
iteration 620 :: loss 0.20578505822304358
iteration 630 :: loss 0.20462507837797883
iteration 640 :: loss 0.20349799865587845
iteration 650 :: loss 0.20240326850698986
iteration 660 :: loss 0.2013386393412988
iteration 670 :: loss 0.20030187904688532
iteration 680 :: loss 0.1992938658199389
iteration 690 :: loss 0.19831341653346052
iteration 700 :: loss 0.19736016076443563
iteration 710 :: loss 0.19643343138503666
iteration 720 :: loss 0.19553153191873873

test accuracy: 0.9668
train accuracy: 0.9733
