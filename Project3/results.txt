Answer2 

a)

iteration 0 :: loss 2.5940543069617954
iteration 10 :: loss 1.66027152005014
iteration 20 :: loss 1.3408146796519456
iteration 30 :: loss 1.0293583112820996
iteration 40 :: loss 0.8434751018711674
iteration 50 :: loss 0.747715206574926
iteration 60 :: loss 0.6987483957552709
iteration 70 :: loss 0.6579854632260809
iteration 80 :: loss 0.6185117688765488
iteration 90 :: loss 0.5897201472236744
iteration 100 :: loss 0.5687253541322465
iteration 110 :: loss 0.5524859496572386
iteration 120 :: loss 0.5392808783351006
iteration 130 :: loss 0.5281741406866634
iteration 140 :: loss 0.5186001356868121
iteration 150 :: loss 0.5102307359885822
iteration 160 :: loss 0.5028184896310421
iteration 170 :: loss 0.4961893892974345
iteration 180 :: loss 0.49022575256837625
iteration 190 :: loss 0.4848201614362613
iteration 200 :: loss 0.4798862678931431
iteration 210 :: loss 0.475354803739916
iteration 220 :: loss 0.471175296456505
iteration 230 :: loss 0.4673045674986106
iteration 240 :: loss 0.46370211714815845
iteration 250 :: loss 0.4603233746841243
iteration 260 :: loss 0.4571476715072709
iteration 270 :: loss 0.4541567913321532
iteration 280 :: loss 0.45133241433388627
iteration 290 :: loss 0.4486535172108886
iteration 300 :: loss 0.44609411247272157
iteration 310 :: loss 0.44365353662347806
iteration 320 :: loss 0.4413200319134444
iteration 330 :: loss 0.43909048458474337
iteration 340 :: loss 0.43695330974705554
iteration 350 :: loss 0.4348931330999727
iteration 360 :: loss 0.4329021847800325
iteration 370 :: loss 0.43098028498585794
iteration 380 :: loss 0.4291233287697773
iteration 390 :: loss 0.4273325229489547
iteration 400 :: loss 0.4255995357224782
iteration 410 :: loss 0.423918910324068
iteration 420 :: loss 0.42229181494793266
iteration 430 :: loss 0.42070874225000643
iteration 440 :: loss 0.41916367266852356
iteration 450 :: loss 0.41765853956633214
iteration 460 :: loss 0.41618350311877217
iteration 470 :: loss 0.41473708432110135
iteration 480 :: loss 0.41332068746520206
iteration 490 :: loss 0.4119344726963093
iteration 500 :: loss 0.4105747171711387
iteration 510 :: loss 0.40924496801571775
iteration 520 :: loss 0.4079404701803976
iteration 530 :: loss 0.4066606365237418
iteration 540 :: loss 0.4054052208658429
iteration 550 :: loss 0.4041691236355091
iteration 560 :: loss 0.4029549880371662
iteration 570 :: loss 0.4017637597845452
iteration 580 :: loss 0.40059312335624137
iteration 590 :: loss 0.39944038114555047
iteration 600 :: loss 0.39830549449054325
iteration 610 :: loss 0.3971889954871366
iteration 620 :: loss 0.3960876644488256
iteration 630 :: loss 0.39499899247465053
iteration 640 :: loss 0.3939236972321455
iteration 650 :: loss 0.3928600317589662
iteration 660 :: loss 0.39181076772707624
iteration 670 :: loss 0.39077627796294134
iteration 680 :: loss 0.38975656235719164
iteration 690 :: loss 0.38874756455256765
iteration 700 :: loss 0.3877499429289797
iteration 710 :: loss 0.38676181998001913
iteration 720 :: loss 0.3857827478753326
iteration 730 :: loss 0.3848144245621961
iteration 740 :: loss 0.3838563424816015
iteration 750 :: loss 0.38291032761117877
iteration 760 :: loss 0.38197362304006177
iteration 770 :: loss 0.3810447974880842
iteration 780 :: loss 0.3801279410454709
iteration 790 :: loss 0.3792199839536094

test accuracy: 0.9289
train accuracy: 0.9275166666666667




ans2 
c
Enable Regularisation
teration 0 :: loss 2.5539534461679545
iteration 10 :: loss 1.6313889902747785
iteration 20 :: loss 1.2019307095735217
iteration 30 :: loss 1.0968373184364104
iteration 40 :: loss 0.8601724983039786
iteration 50 :: loss 0.7758204139959332
iteration 60 :: loss 0.7128831396369336
iteration 70 :: loss 0.6640028986859877
iteration 80 :: loss 0.6289109610553592
iteration 90 :: loss 0.6017164314354142
iteration 100 :: loss 0.5797504883988531
iteration 110 :: loss 0.5617962099903131
iteration 120 :: loss 0.546983635685346
iteration 130 :: loss 0.5345898903189659
iteration 140 :: loss 0.5240416779440714
iteration 150 :: loss 0.5148818906142197
iteration 160 :: loss 0.5068108025921895
iteration 170 :: loss 0.49962506380535654
iteration 180 :: loss 0.49317793414227185
iteration 190 :: loss 0.48734633169204444
iteration 200 :: loss 0.4820328584328741
iteration 210 :: loss 0.4771649426651492
iteration 220 :: loss 0.47268617212471076
iteration 230 :: loss 0.4685397254841197
iteration 240 :: loss 0.4646805363670775
iteration 250 :: loss 0.46107184503718246
iteration 260 :: loss 0.4576895829047506
iteration 270 :: loss 0.4545132028452016
iteration 280 :: loss 0.4515140240393578
iteration 290 :: loss 0.4486734256743632
iteration 300 :: loss 0.44597569426608163
iteration 310 :: loss 0.44340889474581463
iteration 320 :: loss 0.44095797378247326
iteration 330 :: loss 0.43861600163334724
iteration 340 :: loss 0.43637333717899346
iteration 350 :: loss 0.43421886967071793
iteration 360 :: loss 0.4321469820263454
iteration 370 :: loss 0.4301483134720614
iteration 380 :: loss 0.4282191349155418
iteration 390 :: loss 0.4263578411541684
iteration 400 :: loss 0.42455481942640616
iteration 410 :: loss 0.4228078679023216
iteration 420 :: loss 0.4211115372498858
iteration 430 :: loss 0.4194606397130204
iteration 440 :: loss 0.4178503494719301
iteration 450 :: loss 0.4162810952169328
iteration 460 :: loss 0.4147505770002262
iteration 470 :: loss 0.41325250716633066
iteration 480 :: loss 0.41178385886495483
iteration 490 :: loss 0.410349220446125
iteration 500 :: loss 0.4089467696505919
iteration 510 :: loss 0.40757157975116093
iteration 520 :: loss 0.4062248077796355
iteration 530 :: loss 0.40490372338954833
iteration 540 :: loss 0.4036087860456844
iteration 550 :: loss 0.4023337561119883
iteration 560 :: loss 0.40108233583437036
iteration 570 :: loss 0.39985031981821334
iteration 580 :: loss 0.3986392852042768
iteration 590 :: loss 0.39745025918744953
iteration 600 :: loss 0.3962779917723118
iteration 610 :: loss 0.39511796501097246
iteration 620 :: loss 0.3939734192776393
iteration 630 :: loss 0.3928443255314937
iteration 640 :: loss 0.3917286043973377
iteration 650 :: loss 0.3906283328743386
iteration 660 :: loss 0.3895393003237656
iteration 670 :: loss 0.3884636255237982
iteration 680 :: loss 0.3874015714373552
iteration 690 :: loss 0.38635015090117175
iteration 700 :: loss 0.3853089122855
iteration 710 :: loss 0.3842789036481645
iteration 720 :: loss 0.3832584770651674
iteration 730 :: loss 0.38224668105630377
iteration 740 :: loss 0.3812426090890759
iteration 750 :: loss 0.3802483114163307
iteration 760 :: loss 0.37926327164357426
iteration 770 :: loss 0.3782831523242724
iteration 780 :: loss 0.3773103932277719
iteration 790 :: loss 0.37634352782217007
iteration 800 :: loss 0.37538926899040487
iteration 810 :: loss 0.3744446043827684
iteration 820 :: loss 0.3735055404262717
iteration 830 :: loss 0.37257381118089106
iteration 840 :: loss 0.3716501585502838
iteration 850 :: loss 0.37073262543658025
iteration 860 :: loss 0.3698231876238034
iteration 870 :: loss 0.3689219083954388
iteration 880 :: loss 0.36802662721227686
iteration 890 :: loss 0.36713667382815196
iteration 900 :: loss 0.366254140154223
iteration 910 :: loss 0.3653775848866629

test accuracy: 0.9291
train accuracy: 0.9275


Enable both regularisation and momentum

iteration 0 :: loss 2.5617856828048344
iteration 10 :: loss 0.8260308025030689
iteration 20 :: loss 0.5699418559155486
iteration 30 :: loss 0.5069271381037032
iteration 40 :: loss 0.4664178145926738
iteration 50 :: loss 0.44050862192365337
iteration 60 :: loss 0.42325927092178206
iteration 70 :: loss 0.4109677528359804
iteration 80 :: loss 0.40076065820463447
iteration 90 :: loss 0.3916850082414576
iteration 100 :: loss 0.38326191771064244
iteration 110 :: loss 0.3752715185813964
iteration 120 :: loss 0.36760581935444947
iteration 130 :: loss 0.36022365924770094
iteration 140 :: loss 0.35308793355572077
iteration 150 :: loss 0.3462090066700228
iteration 160 :: loss 0.3396011643188224
iteration 170 :: loss 0.3332456783249395
iteration 180 :: loss 0.32714432594901105
iteration 190 :: loss 0.3212799579318193
iteration 200 :: loss 0.3156635527124032
iteration 210 :: loss 0.31028521273220977
iteration 220 :: loss 0.3051471876291743
iteration 230 :: loss 0.30025261279512994
iteration 240 :: loss 0.29557983157260803
iteration 250 :: loss 0.29112958484072776
iteration 260 :: loss 0.28687891797368215
iteration 270 :: loss 0.28281533082474386
iteration 280 :: loss 0.2789268865399298
iteration 290 :: loss 0.2752069388712537
iteration 300 :: loss 0.27164157543487233
iteration 310 :: loss 0.26822510604535804
iteration 320 :: loss 0.2649454502084874
iteration 330 :: loss 0.2617982136344377
iteration 340 :: loss 0.25877246595974773
iteration 350 :: loss 0.2558534246926922
iteration 360 :: loss 0.2530383346256793
iteration 370 :: loss 0.2503234178217866
iteration 380 :: loss 0.24770871013803183
iteration 390 :: loss 0.24519634158700299
iteration 400 :: loss 0.24278012560295464
iteration 410 :: loss 0.24045559914938783
iteration 420 :: loss 0.23821436268312282
iteration 430 :: loss 0.23605355169999329
iteration 440 :: loss 0.2339592939810218
iteration 450 :: loss 0.2319293009798825
iteration 460 :: loss 0.2299678883917235
iteration 470 :: loss 0.2280719314036368
iteration 480 :: loss 0.2262417193739471
iteration 490 :: loss 0.22446768452242583
iteration 500 :: loss 0.22274727625029428
iteration 510 :: loss 0.22108067862256076
iteration 520 :: loss 0.2194652315896563
iteration 530 :: loss 0.2179007587834758
iteration 540 :: loss 0.21638419537084314
iteration 550 :: loss 0.21491594644338755
iteration 560 :: loss 0.21349284553455836
iteration 570 :: loss 0.21211086749885494
iteration 580 :: loss 0.210770828759722
iteration 590 :: loss 0.20946939016922017
iteration 600 :: loss 0.20820578323365566
iteration 610 :: loss 0.2069778013105359
iteration 620 :: loss 0.20578505822304358
iteration 630 :: loss 0.20462507837797883
iteration 640 :: loss 0.20349799865587845
iteration 650 :: loss 0.20240326850698986
iteration 660 :: loss 0.2013386393412988
iteration 670 :: loss 0.20030187904688532
iteration 680 :: loss 0.1992938658199389
iteration 690 :: loss 0.19831341653346052
iteration 700 :: loss 0.19736016076443563
iteration 710 :: loss 0.19643343138503666
iteration 720 :: loss 0.19553153191873873

test accuracy: 0.9668
train accuracy: 0.9733



Answer3 

3_5
Learning rate = 0.01
SGD + nestrov

 
X_train shape: (50000, 3, 32, 32)
50000 train samples
10000 test samples
Using real-time data augmentation.
Epoch 1/200
50000/50000 [==============================] - 617s - loss: 1.7342 - acc: 0.3586 - val_loss: 1.3371 - val_acc: 0.5213
Epoch 2/200
50000/50000 [==============================] - 736s - loss: 1.3719 - acc: 0.5034 - val_loss: 1.1182 - val_acc: 0.5932
Epoch 3/200
50000/50000 [==============================] - 736s - loss: 1.1924 - acc: 0.5757 - val_loss: 0.9900 - val_acc: 0.6505
Epoch 4/200
50000/50000 [==============================] - 657s - loss: 1.0979 - acc: 0.6100 - val_loss: 0.8754 - val_acc: 0.6926
Epoch 5/200
50000/50000 [==============================] - 704s - loss: 1.0436 - acc: 0.6323 - val_loss: 0.9027 - val_acc: 0.6884
Epoch 6/200
50000/50000 [==============================] - 639s - loss: 1.0023 - acc: 0.6457 - val_loss: 0.8304 - val_acc: 0.7057
Epoch 7/200
50000/50000 [==============================] - 769s - loss: 0.9658 - acc: 0.6599 - val_loss: 0.8077 - val_acc: 0.7148
Epoch 8/200
50000/50000 [==============================] - 596s - loss: 0.9374 - acc: 0.6717 - val_loss: 0.7692 - val_acc: 0.7326
Epoch 9/200
50000/50000 [==============================] - 572s - loss: 0.9273 - acc: 0.6752 - val_loss: 0.7406 - val_acc: 0.7457
Epoch 10/200
50000/50000 [==============================] - 721s - loss: 0.9088 - acc: 0.6824 - val_loss: 0.7222 - val_acc: 0.7485
Epoch 11/200
50000/50000 [==============================] - 589s - loss: 0.8902 - acc: 0.6917 - val_loss: 0.7008 - val_acc: 0.7579
Epoch 12/200
50000/50000 [==============================] - 702s - loss: 0.8745 - acc: 0.6964 - val_loss: 0.7158 - val_acc: 0.7503
Epoch 13/200
50000/50000 [==============================] - 572s - loss: 0.8761 - acc: 0.6959 - val_loss: 0.6854 - val_acc: 0.7599
Epoch 14/200
50000/50000 [==============================] - 562s - loss: 0.8589 - acc: 0.6993 - val_loss: 0.6814 - val_acc: 0.7631



SGD only (3_1)

Learning Rate = 0.1

X_train shape: (50000, 3, 32, 32)
50000 train samples
10000 test samples
Using real-time data augmentation.
Epoch 1/200
50000/50000 [==============================] - 495s - loss: 1.8857 - acc: 0.3013 - val_loss: 1.6576 - val_acc: 0.3842
Epoch 2/200
50000/50000 [==============================] - 505s - loss: 1.5022 - acc: 0.4532 - val_loss: 1.3188 - val_acc: 0.5275
Epoch 3/200
50000/50000 [==============================] - 494s - loss: 1.3292 - acc: 0.5231 - val_loss: 1.1969 - val_acc: 0.5717
Epoch 4/200
50000/50000 [==============================] - 508s - loss: 1.2091 - acc: 0.5700 - val_loss: 1.1451 - val_acc: 0.6008
Epoch 5/200
50000/50000 [==============================] - 504s - loss: 1.1230 - acc: 0.6040 - val_loss: 1.0221 - val_acc: 0.6429
Epoch 6/200
50000/50000 [==============================] - 501s - loss: 1.0676 - acc: 0.6248 - val_loss: 1.1216 - val_acc: 0.6201
Epoch 7/200
50000/50000 [==============================] - 586s - loss: 1.0311 - acc: 0.6369 - val_loss: 0.9843 - val_acc: 0.6559
Epoch 8/200
50000/50000 [==============================] - 590s - loss: 1.0073 - acc: 0.6474 - val_loss: 0.9814 - val_acc: 0.6667
Epoch 9/200
50000/50000 [==============================] - 547s - loss: 0.9792 - acc: 0.6571 - val_loss: 0.8414 - val_acc: 0.7111
Epoch 10/200
50000/50000 [==============================] - 987s - loss: 0.9585 - acc: 0.6649 - val_loss: 0.8105 - val_acc: 0.7180
Epoch 11/200
50000/50000 [==============================] - 1335s - loss: 0.9466 - acc: 0.6700 - val_loss: 0.8638 - val_acc: 0.7053
Epoch 12/200
50000/50000 [==============================] - 636s - loss: 0.9333 - acc: 0.6742 - val_loss: 0.8053 - val_acc: 0.7283
Epoch 13/200
50000/50000 [==============================] - 608s - loss: 0.9289 - acc: 0.6756 - val_loss: 0.7886 - val_acc: 0.7285
Epoch 14/200
50000/50000 [==============================] - 586s - loss: 0.9102 - acc: 0.6834 - val_loss: 0.8751 - val_acc: 0.6953
Epoch 15/200
50000/50000 [==============================] - 534s - loss: 0.9081 - acc: 0.6853 - val_loss: 0.7403 - val_acc: 0.7445


answer 3_4

X_train shape: (50000, 3, 32, 32)
50000 train samples
10000 test samples
Using real-time data augmentation.
Epoch 1/200
50000/50000 [==============================] - 621s - loss: 1.6304 - acc: 0.4022 - val_loss: 1.2141 - val_acc: 0.5635
Epoch 2/200
50000/50000 [==============================] - 661s - loss: 1.2596 - acc: 0.5473 - val_loss: 1.0309 - val_acc: 0.6298
Epoch 3/200
50000/50000 [==============================] - 592s - loss: 1.1109 - acc: 0.6031 - val_loss: 0.9090 - val_acc: 0.6803
Epoch 4/200
50000/50000 [==============================] - 598s - loss: 1.0273 - acc: 0.6374 - val_loss: 0.8472 - val_acc: 0.7035
Epoch 5/200
50000/50000 [==============================] - 560s - loss: 0.9648 - acc: 0.6597 - val_loss: 0.8023 - val_acc: 0.7185
Epoch 6/200
50000/50000 [==============================] - 548s - loss: 0.9267 - acc: 0.6725 - val_loss: 0.7575 - val_acc: 0.7379
Epoch 7/200
50000/50000 [==============================] - 585s - loss: 0.8916 - acc: 0.6860 - val_loss: 0.7335 - val_acc: 0.7481
Epoch 8/200
50000/50000 [==============================] - 620s - loss: 0.8656 - acc: 0.6955 - val_loss: 0.7176 - val_acc: 0.7533
Epoch 9/200
50000/50000 [==============================] - 623s - loss: 0.8390 - acc: 0.7070 - val_loss: 0.7072 - val_acc: 0.7594
Epoch 10/200
50000/50000 [==============================] - 595s - loss: 0.8235 - acc: 0.7118 - val_loss: 0.6912 - val_acc: 0.7636
Epoch 11/200
50000/50000 [==============================] - 888s - loss: 0.8075 - acc: 0.7175 - val_loss: 0.6628 - val_acc: 0.7737
Epoch 12/200
50000/50000 [==============================] - 668s - loss: 0.7949 - acc: 0.7202 - val_loss: 0.6677 - val_acc: 0.7713
Epoch 13/200
50000/50000 [==============================] - 750s - loss: 0.7810 - acc: 0.7283 - val_loss: 0.6435 - val_acc: 0.7815
Epoch 14/200
50000/50000 [==============================] - 731s - loss: 0.7674 - acc: 0.7319 - val_loss: 0.6399 - val_acc: 0.7829


answer 3_6

rms = RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)
X_train shape: (50000, 3, 32, 32)
50000 train samples
10000 test samples
Using real-time data augmentation.
Epoch 1/200
50000/50000 [==============================] - 630s - loss: 1.6108 - acc: 0.4158 - val_loss: 1.3289 - val_acc: 0.5108
Epoch 2/200
50000/50000 [==============================] - 649s - loss: 1.2059 - acc: 0.5722 - val_loss: 0.9817 - val_acc: 0.6575
Epoch 3/200
50000/50000 [==============================] - 754s - loss: 1.0686 - acc: 0.6261 - val_loss: 0.8583 - val_acc: 0.7004
Epoch 4/200
50000/50000 [==============================] - 780s - loss: 0.9953 - acc: 0.6528 - val_loss: 0.8485 - val_acc: 0.7063
Epoch 5/200
50000/50000 [==============================] - 666s - loss: 0.9536 - acc: 0.6702 - val_loss: 0.7824 - val_acc: 0.7315
Epoch 6/200
50000/50000 [==============================] - 561s - loss: 0.9273 - acc: 0.6818 - val_loss: 0.7863 - val_acc: 0.7323
Epoch 7/200
50000/50000 [==============================] - 609s - loss: 0.9212 - acc: 0.6854 - val_loss: 0.7733 - val_acc: 0.7417
Epoch 8/200
50000/50000 [==============================] - 658s - loss: 0.9135 - acc: 0.6921 - val_loss: 0.7972 - val_acc: 0.7315
Epoch 9/200
50000/50000 [==============================] - 718s - loss: 0.9179 - acc: 0.6919 - val_loss: 0.7571 - val_acc: 0.7427
Epoch 10/200
50000/50000 [==============================] - 847s - loss: 0.9248 - acc: 0.6934 - val_loss: 0.9051 - val_acc: 0.7028
Epoch 11/200
50000/50000 [==============================] - 754s - loss: 0.9277 - acc: 0.6934 - val_loss: 0.8238 - val_acc: 0.7350
Epoch 12/200
50000/50000 [==============================] - 944s - loss: 0.9520 - acc: 0.6898 - val_loss: 0.8225 - val_acc: 0.7326
Epoch 13/200
50000/50000 [==============================] - 871s - loss: 0.9675 - acc: 0.6853 - val_loss: 0.8210 - val_acc: 0.7314



Ansswer 2 (b)

iteration 0: loss 2.462944
iteration 10: loss 1.763844
iteration 20: loss 1.453537
iteration 30: loss 1.206578
iteration 40: loss 0.941039
iteration 50: loss 1.108595
iteration 60: loss 0.772467
iteration 70: loss 0.718412
iteration 80: loss 0.646285
iteration 90: loss 0.622487
iteration 100: loss 0.636004
iteration 110: loss 0.594227
iteration 120: loss 0.540784
iteration 130: loss 0.539862
iteration 140: loss 0.511598
iteration 150: loss 0.487001
iteration 160: loss 0.479319
iteration 170: loss 0.484740
iteration 180: loss 0.481151
iteration 190: loss 0.459909
iteration 200: loss 0.449254
iteration 210: loss 0.443194
iteration 220: loss 0.440368
iteration 230: loss 0.443535
iteration 240: loss 0.446605
iteration 250: loss 0.432056
iteration 260: loss 0.424137
iteration 270: loss 0.419081
iteration 280: loss 0.414699
iteration 290: loss 0.410880
iteration 300: loss 0.407610
iteration 310: loss 0.404689
iteration 320: loss 0.401991
iteration 330: loss 0.399370
iteration 340: loss 0.396617
iteration 350: loss 0.393729
iteration 360: loss 0.390791
iteration 370: loss 0.387939
iteration 380: loss 0.385282
iteration 390: loss 0.382767
iteration 400: loss 0.380419
iteration 410: loss 0.378182
iteration 420: loss 0.376064
iteration 430: loss 0.374004
iteration 440: loss 0.371991
iteration 450: loss 0.370034
iteration 460: loss 0.368094
iteration 470: loss 0.366179
iteration 480: loss 0.364280
iteration 490: loss 0.362377
iteration 500: loss 0.360481
iteration 510: loss 0.358572
iteration 520: loss 0.356698
iteration 530: loss 0.354821
iteration 540: loss 0.352927
iteration 550: loss 0.351053
iteration 560: loss 0.349281
iteration 570: loss 0.347592
iteration 580: loss 0.345965
iteration 590: loss 0.344388
iteration 600: loss 0.342832
iteration 610: loss 0.341295
iteration 620: loss 0.339755
iteration 630: loss 0.338214
iteration 640: loss 0.336706
iteration 650: loss 0.335231
iteration 660: loss 0.333764
iteration 670: loss 0.332308
iteration 680: loss 0.330881
iteration 690: loss 0.329486
iteration 700: loss 0.328106
iteration 710: loss 0.326741
iteration 720: loss 0.325409
iteration 730: loss 0.324076
iteration 740: loss 0.322746
iteration 750: loss 0.321428
iteration 760: loss 0.320135
iteration 770: loss 0.318855
iteration 780: loss 0.317591
iteration 790: loss 0.316358
iteration 800: loss 0.315152
iteration 810: loss 0.313958
iteration 820: loss 0.312780
iteration 830: loss 0.311621
iteration 840: loss 0.310503
iteration 850: loss 0.309406
iteration 860: loss 0.308307
iteration 870: loss 0.307203
iteration 880: loss 0.306123
iteration 890: loss 0.305059
iteration 900: loss 0.304001
iteration 910: loss 0.302941
iteration 920: loss 0.301886
iteration 930: loss 0.300806
iteration 940: loss 0.299727
iteration 950: loss 0.298656
iteration 960: loss 0.297597
iteration 970: loss 0.296553
iteration 980: loss 0.295514
iteration 990: loss 0.294475
iteration 1000: loss 0.293429
iteration 1010: loss 0.292402
iteration 1020: loss 0.291404
iteration 1030: loss 0.290412
iteration 1040: loss 0.289442
iteration 1050: loss 0.288491
iteration 1060: loss 0.287566
iteration 1070: loss 0.286661
iteration 1080: loss 0.285777
iteration 1090: loss 0.284888
iteration 1100: loss 0.283986
iteration 1110: loss 0.283091
iteration 1120: loss 0.282202
iteration 1130: loss 0.281319
iteration 1140: loss 0.280430
iteration 1150: loss 0.279556
iteration 1160: loss 0.278686
iteration 1170: loss 0.277821


