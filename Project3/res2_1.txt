Answer2 

a)
0   :: 2.5940543069617954
10  :: 1.66027152005014
20  :: 1.3408146796519456
30  :: 1.0293583112820996
40  :: 0.8434751018711674
50  :: 0.747715206574926
60  :: 0.6987483957552709
70  :: 0.6579854632260809
80  :: 0.6185117688765488
90  :: 0.5897201472236744
100 :: 0.5687253541322465
110 :: 0.5524859496572386
120 :: 0.5392808783351006
130 :: 0.5281741406866634
140 :: 0.5186001356868121
150 :: 0.5102307359885822
160 :: 0.5028184896310421
170 :: 0.4961893892974345
180 :: 0.49022575256837625
190 :: 0.4848201614362613
200 :: 0.4798862678931431
210 :: 0.475354803739916
220 :: 0.471175296456505
230 :: 0.4673045674986106
240 :: 0.46370211714815845
250 :: 0.4603233746841243
260 :: 0.4571476715072709
270 :: 0.4541567913321532
280 :: 0.45133241433388627
290 :: 0.4486535172108886
300 :: 0.44609411247272157
310 :: 0.44365353662347806
320 :: 0.4413200319134444
330 :: 0.43909048458474337
340 :: 0.43695330974705554
350 :: 0.4348931330999727
360 :: 0.4329021847800325
370 :: 0.43098028498585794
380 :: 0.4291233287697773
390 :: 0.4273325229489547
400 :: 0.4255995357224782
410 :: 0.423918910324068
420 :: 0.42229181494793266
430 :: 0.42070874225000643
440 :: 0.41916367266852356
450 :: 0.41765853956633214
460 :: 0.41618350311877217
470 :: 0.41473708432110135
480 :: 0.41332068746520206
490 :: 0.4119344726963093
500 :: 0.4105747171711387
510 :: 0.40924496801571775
520 :: 0.4079404701803976
530 :: 0.4066606365237418
540 :: 0.4054052208658429
550 :: 0.4041691236355091
560 :: 0.4029549880371662
570 :: 0.4017637597845452
580 :: 0.40059312335624137
590 :: 0.39944038114555047
600 :: 0.39830549449054325
610 :: 0.3971889954871366
620 :: 0.3960876644488256
630 :: 0.39499899247465053
640 :: 0.3939236972321455
650 :: 0.3928600317589662
660 :: 0.39181076772707624
670 :: 0.39077627796294134
680 :: 0.38975656235719164
690 :: 0.38874756455256765
700 :: 0.3877499429289797
710 :: 0.38676181998001913
720 :: 0.3857827478753326
730 :: 0.3848144245621961
740 :: 0.3838563424816015
750 :: 0.38291032761117877
760 :: 0.38197362304006177
770 :: 0.3810447974880842
780 :: 0.3801279410454709
790 :: 0.3792199839536094

test accuracy: 0.9289
train accuracy: 0.9275166666666667




ans2 
c
Enable Regularisation
teration 0 :: 2.5539534461679545
10 :: 1.6313889902747785
20 :: 1.2019307095735217
30 :: 1.0968373184364104
40 :: 0.8601724983039786
50 :: 0.7758204139959332
60 :: 0.7128831396369336
70 :: 0.6640028986859877
80 :: 0.6289109610553592
90 :: 0.6017164314354142
100 :: 0.5797504883988531
110 :: 0.5617962099903131
120 :: 0.546983635685346
130 :: 0.5345898903189659
140 :: 0.5240416779440714
150 :: 0.5148818906142197
160 :: 0.5068108025921895
170 :: 0.49962506380535654
180 :: 0.49317793414227185
190 :: 0.48734633169204444
200 :: 0.4820328584328741
210 :: 0.4771649426651492
220 :: 0.47268617212471076
230 :: 0.4685397254841197
240 :: 0.4646805363670775
250 :: 0.46107184503718246
260 :: 0.4576895829047506
270 :: 0.4545132028452016
280 :: 0.4515140240393578
290 :: 0.4486734256743632
300 :: 0.44597569426608163
310 :: 0.44340889474581463
320 :: 0.44095797378247326
330 :: 0.43861600163334724
340 :: 0.43637333717899346
350 :: 0.43421886967071793
360 :: 0.4321469820263454
370 :: 0.4301483134720614
380 :: 0.4282191349155418
390 :: 0.4263578411541684
400 :: 0.42455481942640616
410 :: 0.4228078679023216
420 :: 0.4211115372498858
430 :: 0.4194606397130204
440 :: 0.4178503494719301
450 :: 0.4162810952169328
460 :: 0.4147505770002262
470 :: 0.41325250716633066
480 :: 0.41178385886495483
490 :: 0.410349220446125
500 :: 0.4089467696505919
510 :: 0.40757157975116093
520 :: 0.4062248077796355
530 :: 0.40490372338954833
540 :: 0.4036087860456844
550 :: 0.4023337561119883
560 :: 0.40108233583437036
570 :: 0.39985031981821334
580 :: 0.3986392852042768
590 :: 0.39745025918744953
600 :: 0.3962779917723118
610 :: 0.39511796501097246
620 :: 0.3939734192776393
630 :: 0.3928443255314937
640 :: 0.3917286043973377
650 :: 0.3906283328743386
660 :: 0.3895393003237656
670 :: 0.3884636255237982
680 :: 0.3874015714373552
690 :: 0.38635015090117175
700 :: 0.3853089122855
710 :: 0.3842789036481645
720 :: 0.3832584770651674
730 :: 0.38224668105630377
740 :: 0.3812426090890759
750 :: 0.3802483114163307
760 :: 0.37926327164357426
770 :: 0.3782831523242724
780 :: 0.3773103932277719
790 :: 0.37634352782217007
800 :: 0.37538926899040487
810 :: 0.3744446043827684
820 :: 0.3735055404262717
830 :: 0.37257381118089106
840 :: 0.3716501585502838
850 :: 0.37073262543658025
860 :: 0.3698231876238034
870 :: 0.3689219083954388
880 :: 0.36802662721227686
890 :: 0.36713667382815196
900 :: 0.366254140154223
910 :: 0.3653775848866629

test accuracy: 0.9291
train accuracy: 0.9275


Enable both regularisation and momentum

0 :: 2.5617856828048344
10 :: 0.8260308025030689
20 :: 0.5699418559155486
30 :: 0.5069271381037032
40 :: 0.4664178145926738
50 :: 0.44050862192365337
60 :: 0.42325927092178206
70 :: 0.4109677528359804
80 :: 0.40076065820463447
90 :: 0.3916850082414576
100 :: 0.38326191771064244
110 :: 0.3752715185813964
120 :: 0.36760581935444947
130 :: 0.36022365924770094
140 :: 0.35308793355572077
150 :: 0.3462090066700228
160 :: 0.3396011643188224
170 :: 0.3332456783249395
180 :: 0.32714432594901105
190 :: 0.3212799579318193
200 :: 0.3156635527124032
210 :: 0.31028521273220977
220 :: 0.3051471876291743
230 :: 0.30025261279512994
240 :: 0.29557983157260803
250 :: 0.29112958484072776
260 :: 0.28687891797368215
270 :: 0.28281533082474386
280 :: 0.2789268865399298
290 :: 0.2752069388712537
300 :: 0.27164157543487233
310 :: 0.26822510604535804
320 :: 0.2649454502084874
330 :: 0.2617982136344377
340 :: 0.25877246595974773
350 :: 0.2558534246926922
360 :: 0.2530383346256793
370 :: 0.2503234178217866
380 :: 0.24770871013803183
390 :: 0.24519634158700299
400 :: 0.24278012560295464
410 :: 0.24045559914938783
420 :: 0.23821436268312282
430 :: 0.23605355169999329
440 :: 0.2339592939810218
450 :: 0.2319293009798825
460 :: 0.2299678883917235
470 :: 0.2280719314036368
480 :: 0.2262417193739471
490 :: 0.22446768452242583
500 :: 0.22274727625029428
510 :: 0.22108067862256076
520 :: 0.2194652315896563
530 :: 0.2179007587834758
540 :: 0.21638419537084314
550 :: 0.21491594644338755
560 :: 0.21349284553455836
570 :: 0.21211086749885494
580 :: 0.210770828759722
590 :: 0.20946939016922017
600 :: 0.20820578323365566
610 :: 0.2069778013105359
620 :: 0.20578505822304358
630 :: 0.20462507837797883
640 :: 0.20349799865587845
650 :: 0.20240326850698986
660 :: 0.2013386393412988
670 :: 0.20030187904688532
680 :: 0.1992938658199389
690 :: 0.19831341653346052
700 :: 0.19736016076443563
710 :: 0.19643343138503666
720 :: 0.19553153191873873

test accuracy: 0.9668
train accuracy: 0.9733



Answer3 

3_5
Learning rate = 0.01
SGD + nestrov

 
X_train shape: (50000, 3, 32, 32)
50000 train samples
10000 test samples
Using real-time data augmentation.
Epoch 1/200
50000/50000 [==============================] - 617s - loss: 1.7342 - acc: 0.3586 - val_loss: 1.3371 - val_acc: 0.5213
Epoch 2/200
50000/50000 [==============================] - 736s - loss: 1.3719 - acc: 0.5034 - val_loss: 1.1182 - val_acc: 0.5932
Epoch 3/200
50000/50000 [==============================] - 736s - loss: 1.1924 - acc: 0.5757 - val_loss: 0.9900 - val_acc: 0.6505
Epoch 4/200
50000/50000 [==============================] - 657s - loss: 1.0979 - acc: 0.6100 - val_loss: 0.8754 - val_acc: 0.6926
Epoch 5/200
50000/50000 [==============================] - 704s - loss: 1.0436 - acc: 0.6323 - val_loss: 0.9027 - val_acc: 0.6884
Epoch 6/200
50000/50000 [==============================] - 639s - loss: 1.0023 - acc: 0.6457 - val_loss: 0.8304 - val_acc: 0.7057
Epoch 7/200
50000/50000 [==============================] - 769s - loss: 0.9658 - acc: 0.6599 - val_loss: 0.8077 - val_acc: 0.7148
Epoch 8/200
50000/50000 [==============================] - 596s - loss: 0.9374 - acc: 0.6717 - val_loss: 0.7692 - val_acc: 0.7326
Epoch 9/200
50000/50000 [==============================] - 572s - loss: 0.9273 - acc: 0.6752 - val_loss: 0.7406 - val_acc: 0.7457
Epoch 10/200
50000/50000 [==============================] - 721s - loss: 0.9088 - acc: 0.6824 - val_loss: 0.7222 - val_acc: 0.7485
Epoch 11/200
50000/50000 [==============================] - 589s - loss: 0.8902 - acc: 0.6917 - val_loss: 0.7008 - val_acc: 0.7579
Epoch 12/200
50000/50000 [==============================] - 702s - loss: 0.8745 - acc: 0.6964 - val_loss: 0.7158 - val_acc: 0.7503
Epoch 13/200
50000/50000 [==============================] - 572s - loss: 0.8761 - acc: 0.6959 - val_loss: 0.6854 - val_acc: 0.7599
Epoch 14/200
50000/50000 [==============================] - 562s - loss: 0.8589 - acc: 0.6993 - val_loss: 0.6814 - val_acc: 0.7631



SGD only (3_1)

Learning Rate = 0.1

X_train shape: (50000, 3, 32, 32)
50000 train samples
10000 test samples
Using real-time data augmentation.
Epoch 1/200
50000/50000 [==============================] - 495s - loss: 1.8857 - acc: 0.3013 - val_loss: 1.6576 - val_acc: 0.3842
Epoch 2/200
50000/50000 [==============================] - 505s - loss: 1.5022 - acc: 0.4532 - val_loss: 1.3188 - val_acc: 0.5275
Epoch 3/200
50000/50000 [==============================] - 494s - loss: 1.3292 - acc: 0.5231 - val_loss: 1.1969 - val_acc: 0.5717
Epoch 4/200
50000/50000 [==============================] - 508s - loss: 1.2091 - acc: 0.5700 - val_loss: 1.1451 - val_acc: 0.6008
Epoch 5/200
50000/50000 [==============================] - 504s - loss: 1.1230 - acc: 0.6040 - val_loss: 1.0221 - val_acc: 0.6429
Epoch 6/200
50000/50000 [==============================] - 501s - loss: 1.0676 - acc: 0.6248 - val_loss: 1.1216 - val_acc: 0.6201
Epoch 7/200
50000/50000 [==============================] - 586s - loss: 1.0311 - acc: 0.6369 - val_loss: 0.9843 - val_acc: 0.6559
Epoch 8/200
50000/50000 [==============================] - 590s - loss: 1.0073 - acc: 0.6474 - val_loss: 0.9814 - val_acc: 0.6667
Epoch 9/200
50000/50000 [==============================] - 547s - loss: 0.9792 - acc: 0.6571 - val_loss: 0.8414 - val_acc: 0.7111
Epoch 10/200
50000/50000 [==============================] - 987s - loss: 0.9585 - acc: 0.6649 - val_loss: 0.8105 - val_acc: 0.7180
Epoch 11/200
50000/50000 [==============================] - 1335s - loss: 0.9466 - acc: 0.6700 - val_loss: 0.8638 - val_acc: 0.7053
Epoch 12/200
50000/50000 [==============================] - 636s - loss: 0.9333 - acc: 0.6742 - val_loss: 0.8053 - val_acc: 0.7283
Epoch 13/200
50000/50000 [==============================] - 608s - loss: 0.9289 - acc: 0.6756 - val_loss: 0.7886 - val_acc: 0.7285
Epoch 14/200
50000/50000 [==============================] - 586s - loss: 0.9102 - acc: 0.6834 - val_loss: 0.8751 - val_acc: 0.6953
Epoch 15/200
50000/50000 [==============================] - 534s - loss: 0.9081 - acc: 0.6853 - val_loss: 0.7403 - val_acc: 0.7445


answer 3_4

X_train shape: (50000, 3, 32, 32)
50000 train samples
10000 test samples
Using real-time data augmentation.
Epoch 1/200
50000/50000 [==============================] - 621s - loss: 1.6304 - acc: 0.4022 - val_loss: 1.2141 - val_acc: 0.5635
Epoch 2/200
50000/50000 [==============================] - 661s - loss: 1.2596 - acc: 0.5473 - val_loss: 1.0309 - val_acc: 0.6298
Epoch 3/200
50000/50000 [==============================] - 592s - loss: 1.1109 - acc: 0.6031 - val_loss: 0.9090 - val_acc: 0.6803
Epoch 4/200
50000/50000 [==============================] - 598s - loss: 1.0273 - acc: 0.6374 - val_loss: 0.8472 - val_acc: 0.7035
Epoch 5/200
50000/50000 [==============================] - 560s - loss: 0.9648 - acc: 0.6597 - val_loss: 0.8023 - val_acc: 0.7185
Epoch 6/200
50000/50000 [==============================] - 548s - loss: 0.9267 - acc: 0.6725 - val_loss: 0.7575 - val_acc: 0.7379
Epoch 7/200
50000/50000 [==============================] - 585s - loss: 0.8916 - acc: 0.6860 - val_loss: 0.7335 - val_acc: 0.7481
Epoch 8/200
50000/50000 [==============================] - 620s - loss: 0.8656 - acc: 0.6955 - val_loss: 0.7176 - val_acc: 0.7533
Epoch 9/200
50000/50000 [==============================] - 623s - loss: 0.8390 - acc: 0.7070 - val_loss: 0.7072 - val_acc: 0.7594
Epoch 10/200
50000/50000 [==============================] - 595s - loss: 0.8235 - acc: 0.7118 - val_loss: 0.6912 - val_acc: 0.7636
Epoch 11/200
50000/50000 [==============================] - 888s - loss: 0.8075 - acc: 0.7175 - val_loss: 0.6628 - val_acc: 0.7737
Epoch 12/200
50000/50000 [==============================] - 668s - loss: 0.7949 - acc: 0.7202 - val_loss: 0.6677 - val_acc: 0.7713
Epoch 13/200
50000/50000 [==============================] - 750s - loss: 0.7810 - acc: 0.7283 - val_loss: 0.6435 - val_acc: 0.7815
Epoch 14/200
50000/50000 [==============================] - 731s - loss: 0.7674 - acc: 0.7319 - val_loss: 0.6399 - val_acc: 0.7829


answer 3_6

rms = RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)
X_train shape: (50000, 3, 32, 32)
50000 train samples
10000 test samples
Using real-time data augmentation.
Epoch 1/200
50000/50000 [==============================] - 630s - loss: 1.6108 - acc: 0.4158 - val_loss: 1.3289 - val_acc: 0.5108
Epoch 2/200
50000/50000 [==============================] - 649s - loss: 1.2059 - acc: 0.5722 - val_loss: 0.9817 - val_acc: 0.6575
Epoch 3/200
50000/50000 [==============================] - 754s - loss: 1.0686 - acc: 0.6261 - val_loss: 0.8583 - val_acc: 0.7004
Epoch 4/200
50000/50000 [==============================] - 780s - loss: 0.9953 - acc: 0.6528 - val_loss: 0.8485 - val_acc: 0.7063
Epoch 5/200
50000/50000 [==============================] - 666s - loss: 0.9536 - acc: 0.6702 - val_loss: 0.7824 - val_acc: 0.7315
Epoch 6/200
50000/50000 [==============================] - 561s - loss: 0.9273 - acc: 0.6818 - val_loss: 0.7863 - val_acc: 0.7323
Epoch 7/200
50000/50000 [==============================] - 609s - loss: 0.9212 - acc: 0.6854 - val_loss: 0.7733 - val_acc: 0.7417
Epoch 8/200
50000/50000 [==============================] - 658s - loss: 0.9135 - acc: 0.6921 - val_loss: 0.7972 - val_acc: 0.7315
Epoch 9/200
50000/50000 [==============================] - 718s - loss: 0.9179 - acc: 0.6919 - val_loss: 0.7571 - val_acc: 0.7427
Epoch 10/200
50000/50000 [==============================] - 847s - loss: 0.9248 - acc: 0.6934 - val_loss: 0.9051 - val_acc: 0.7028
Epoch 11/200
50000/50000 [==============================] - 754s - loss: 0.9277 - acc: 0.6934 - val_loss: 0.8238 - val_acc: 0.7350
Epoch 12/200
50000/50000 [==============================] - 944s - loss: 0.9520 - acc: 0.6898 - val_loss: 0.8225 - val_acc: 0.7326
Epoch 13/200
50000/50000 [==============================] - 871s - loss: 0.9675 - acc: 0.6853 - val_loss: 0.8210 - val_acc: 0.7314


